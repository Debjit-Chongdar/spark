1. each action will create execution plan from action point to start
2. if any shuffle happened between action & start, then it will skip previous stage of suffle (It will be shown in DAG as skipped)
3. -> each action create a new job
4. -> each shuffle between partition, creates a new stage
5. -> each transformation creates a new task
6. Action => reduce(Fn),collect(),count(), take(5), first(), save(), foreach, explain(), (sortByKey(true))
7. Shuffle => reduceByKey(Fn),groupByKey(),sortByKey(false),repartition(6),coalesce(2)
8. Transformation => map(Fn), mapToPair(PairFn), flatMap(Iterator),
9. Join is possible only on JavaPairRdd(key)   # -> return JavaPairRDD<key, Tuple2(1st_val, 2nd_val)>

10. JavaRDD<String> inputRdd = jsc.parallelize() / jsc.textFIle()
    jsc.parallelizePairs(Arrays.asList(new Tuple2<>("a", "abc"),...))
    Dataset<Row> inputDs = session.read().option("header", true).csv(csvPath);

11. sortAggregation is slower than hashAggregation, (HashAggregate is more faster with JavaAPI instead sql on temp table)
            hashAggregation apply if sortBy column is part of groupBy columns
        or  if it's not part of groupBy column but it's mutable type like Integer, Long, Double, Short, etc
                where size is fixed like 32 bit for Integer, 64 bit for Long, not varargs like String.
12. batchInterval (Must be) ≤ windowDuration, windowDuration (Must be) ≥ batchInterval,
        slideDuration Usually equals batchInterval, or a divisor of it

13. Dataset<Row> broadcastDs = functions.broadcast(inputDs);
14. Broadcast<Map<String, Integer>> broadcastMap= jsc.broadcast(pairRDD.collectAsMap());
    Map<String, Integer> map = broadcastMap.value();
    // jsc.broadcast(javaRDD) is not possible as it only allow to broadcast small data
    broadcastDs.unpersist(); Removes from executors, Can reuse later
    broadcastMap.destroy();   Removes from executors and driver, Can't reuse later

15. inputDs = inputDs.cache(); //eq// inputDs = inputDs.persist(StorageLevel.MEMORY_AND_DISK()); //MEMORY_ONLY_SER()
16. inputRDD = inputRDD.cache(); //eq// inputRDD = inputRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());//MEMORY_ONLY()
    inputDs.unpersist(); // false
    inputRDD.unpersist(true); // block until complete

16. Dataset<Row> = ds1.join(ds2, ds1.col("abc").equalTo(ds2.col("abc")), "inner"); // outer // left_outer //
17. JavaPairRDD<String, Tuple2<Double, Integer>> innerJoin = deptAvjSalRdd.join(deptNameRdd); // no optional
18. JavaPairRDD<String, Tuple2<Double, Optional<Integer>>> left = deptAvjSalRdd.leftOuterJoin(deptNameRdd); // one optional
19. JavaPairRDD<String, Tuple2<Optional<Double>, Optional<Integer>>> outer = deptAvjSalRdd.fullOuterJoin(deptNameRdd);

20. inputDs.createOrReplaceTempView("Employee");
    session.catalog().dropTempView("Employee");

21. session.udf().register("empResultUDF", new EmployeeResultUDF2(), DataTypes.StringType); //return type
    "select name, rating, age, empResultUDF(rating, age) as result from emp_view"

    UDF2<String, String, String> resultUdf = (String rating, String age) -> {"test"};
    session.udf().register("resultUDF", resultUdf, DataTypes.StringType);
    Dataset<Row> redultDs1 = empDs.select("name", "rating", "age")
                  .withColumn("result", functions.callUDF("resultUDF", col("rating"), col("age")));

22. Dataset<Row> emplRows = session.createDataFrame(values, new StructType(structFields));
    Dataset<Row> emplRows = session.createDataFrame(empl_list, Emp.class);
23. Encoder<Person> personEncoder = Encoders.bean(Person.class);
    Dataset<Person> personDataset = spark.createDataset(personList, personEncoder);
    or
    Dataset<Emp> emplDs = emplRows.as(Encoders.bean(Emp.class));

24. Salting : Dataset<Long> saltDs = session.range(5); // column name = id

25. Dataset<Row> streamDs = sparkSession.readStream()
                    .format("kafka")
                    .option("kafka.bootstrap.servers", "localhost:9092")
                    .option("subscribe", "test-topic")
                    .load();